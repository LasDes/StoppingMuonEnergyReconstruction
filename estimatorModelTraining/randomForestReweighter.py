# # # # # # # # # # # # # # #
# Random Forest Reweighter  #
# # # # # # # # # # # # # # #
#
# Takes data set A and calculates weight so that the weighted pdf of A
# corresponds to a second data set B.
#
# T. Hoinka

import numpy as np
import sys
from matplotlib import pyplot as plt
from scipy.stats import chisquare
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier
from sklearn.calibration import CalibratedClassifierCV
from sklearn.metrics import roc_auc_score, roc_curve
from sklearn.model_selection import StratifiedKFold
from sklearn.neighbors import NearestNeighbors
from sklearn.preprocessing import MinMaxScaler

def _soft_penalty(x, lower=0.5, upper=2.0):
    return 2.0 * np.arctan(2.0 * x - 2.0) / np.pi + 1.0

def predict_split(forest, x):
    predictions = np.zeros((len(x), forest.n_estimators))
    for estimator, i in zip(forest.estimators_, range(forest.n_estimators)):
        predictions[:, i] = estimator.predict_proba(x)[:, 1]
    rw_factors = (predictions + 1e-3) / (1.0 - predictions + 1e-3)
    return np.prod(rw_factors, axis=1) ** (1.0 / forest.n_estimators)

"""Calculates reweighting factors to assimilate the pdf of data_orig to
data_target using a Random Forest.

Parameters
----------
data_orig : array, shape = [n_samples, n_features]
            Original data set (usually something like your mc data set).

data_target : array, shape = [n_samples, n_features]
              Target data set (usually something like your real data set).

n_iterations : integer, optional
               Number of iterations. More will result in closer fit, but less
               stability.

n_estimators : integer, optional
               Number of estimators to be used by the classifier. More will
               result in worse performance.

max_depth : integer, optional
            Maximum depth of trees generated by the random forest. More will
            greatly reduce stability.

sample : float, optional
         Fraction of samples bootstrapped in each iteration step. More greatly
         reduces performance and stability.

weights_orig : array, shape = [n_samples,], optional
               Weights of the original data set, if available. Can be used to
               continue processing by inserting the output of this function.

reg_power : float, optional
            Regularization power. Lower (< 1.0) will enhance stability.
            Negative values make no sense here and will actually reverse what
            this function is supposed to do, so stick with positive powers.

Returns
-------
rw_factors : array, shape = [n_samples,]
             Resulting reweighting factors.
"""
def calculate_reweighting_factors(data_orig, data_target,
                                  classifier_params={}, sample=1.0,
                                  reg_power=1.0, reg_offset=1.0e-3,
                                  n_iterations=15, weights_orig=None):
    if weights_orig is None:
        rw_factors = np.ones(len(data_orig))
        w_mean = 1.0
    else:
        rw_factors = np.copy(weights_orig)
        w_mean = np.mean(weights_orig)
    orig_sum = np.sum(rw_factors)
    # If both data sets are the same length (and they better be), then all
    # samples are chosen.
    N_all = np.min([len(data_orig), len(data_target)])
    N = int(sample * N_all)
    rf = RandomForestClassifier()
    rf.set_params(**{"n_estimators": 1,
                     "max_depth": 3,
                     "min_samples_leaf": 0.01,
                     "n_jobs": -1})
    rf.set_params(**classifier_params)
    for i in range(n_iterations):
        orig_picks = np.random.permutation(len(data_orig))[:N]
        target_picks = np.random.permutation(len(data_target))[:N]
        training_set = np.concatenate((data_orig[orig_picks, :],
                                       data_target[target_picks, :]))
        training_lab = np.concatenate((np.zeros(len(orig_picks)),
                                       np.ones(len(target_picks))))
        picked_weights = np.concatenate((rw_factors[orig_picks],
                                         w_mean * np.ones(len(target_picks)))) 
        rf.fit(training_set, training_lab, sample_weight=picked_weights)
        pred = predict_split(rf, data_orig)
        #pred[np.abs(1 - 2.0 * pred) < 0.01] = 0.5
        #rw_factors *= ((pred[:, 1] + reg_offset) / (pred[:, 0] + reg_offset)) ** reg_power
        #rw_factors = _soft_penalty(rw_factors / weights_orig) * weights_orig
        #rw_factors *= orig_sum / np.sum(rw_factors)
        rw_factors *= pred
        print("Finished iteration step %i." % i)
    return rw_factors

"""Calculates a measure for the mismatch between two datasets using the AUC
score.

Parameters
----------
data_orig : array, shape = [n_samples, n_features]
            Original data set (usually mc).

data_target : array, shape = [n_samples, n_features]
              Target data set (usually real data).

sample : float, optional
         Fraction of data to be used for the classification.

Returns
-------
auc_score : float
            AUC score as a overall measure for the difference between data_orig
            and data_target.
fpr : array, shape = [>2]
      False Positive Rates.
tpr : array, shape = [>2]
      True Positive Rates.
"""
def calculate_mismatch(data_orig, data_target, sample_weight, cv_splits=5,
                       fraction=1.0):

    training_set = np.concatenate((data_orig, data_target))
    training_lab = np.concatenate((np.zeros(len(data_orig)),
                                   np.ones(len(data_target))))
    w_mean = np.mean(sample_weight)
    weights = np.concatenate((sample_weight,
                              w_mean * np.ones(len(data_target))))
    if fraction != 1.0:
        p = np.random.choice(len(training_set),
                             int(len(training_set) * fraction))
        training_set = training_set[p, :]
        training_lab = training_lab[p]
    splits = StratifiedKFold(n_splits=cv_splits).split(training_set,
                                                       training_lab)
    rf = RandomForestClassifier(n_estimators=100, n_jobs=-1)
    scores = np.zeros(len(training_set))
    auc_scores = np.zeros(cv_splits)
    fprs = []
    tprs = []
    for (train_i, test_i), i in zip(splits, range(cv_splits)):
        rf.fit(training_set[train_i, :], training_lab[train_i],
               sample_weight=weights[train_i])
        scores[test_i] = rf.predict_proba(training_set[test_i, :])[:, 1]
        auc_scores[i] = roc_auc_score(training_lab[test_i], scores[test_i],
                                     sample_weight=weights[test_i])
        fpr, tpr, _ = roc_curve(training_lab[test_i], scores[test_i],
                                sample_weight=weights[test_i])
        fprs += [fpr]
        tprs += [tpr]

    return auc_scores, fprs, tprs, rf.feature_importances_

"""Performs a KS-test. Note: Using weights makes this test useless
for hypothesis tests.

Parameters
----------
p1 : array, shape = [n_samples_1,]
     First dataset.
    
p2 : array, shape = [n_samples_2,]
     Second dataset.
     
w1 : array, shape = [n_samples_1,], optional
     Weights for first dataset.

w2 : array, shape = [n_samples_2,], optional
     Weights for second dataset.

Returns
-------
ks_statistic : float
               KS statistic value.
"""
def ks_test(p1, p2, w1=None, w2=None):
    if w1 is None:
        weight_1 = np.concatenate((np.ones(len(p1)) / float(len(p1)), np.zeros(len(p2))))
    else:
        weight_1 = np.concatenate((w1 / float(np.sum(w1)), np.zeros(len(p2))))
    if w2 is None:
        weight_2 = np.concatenate((np.zeros(len(p1)), np.ones(len(p2)) / float(len(p2))))
    else:
        weight_2 = np.concatenate((np.zeros(len(p1)), w2 / float(np.sum(w2))))
    concat_p = np.concatenate((p1, p2))
    cumsum_1 = np.cumsum(weight_1[np.argsort(concat_p)])
    cumsum_2 = np.cumsum(weight_2[np.argsort(concat_p)])
    ks_statistic = np.max(np.abs(cumsum_1 - cumsum_2))
    factor = np.sqrt(len(p1) * len(p2) / (len(p1) + len(p2)))
    p_value = 2.0 / np.exp(2.0 * (ks_statistic * factor) ** 2)
    return ks_statistic, p_value

"""Performs a *symmetrized* chi squared test.
Note: Using weights makes this test useless for hypothesis tests.

Parameters
----------
p1 : array, shape = [n_samples_1, n_features]
     First dataset.
    
p2 : array, shape = [n_samples_2, n_features]
     Second dataset.
     
w1 : array, shape = [n_samples_1,], optional
     Weights for first dataset.

w2 : array, shape = [n_samples_2,], optional
     Weights for second dataset.

norm : bool, optional
       Whether or not to normalize the datasets.

Returns
-------
chi2_statistic : float
                 chi squared statistic value.
"""
def chi2_test(p1, p2, w1=None, w2=None, norm=False):
    if w1 is None:
        w1 = np.ones(len(p1))
    if w2 is None:
        w2 = np.ones(len(p2))

    w1 /= np.sum(w1)
    w2 /= np.sum(w2)
    if norm is True:
        p1_min = np.min(p1, axis=0)
        p1_max = np.max(p1, axis=0)
        p2_min = np.min(p2, axis=0)
        p2_max = np.max(p2, axis=0)
        p_norm_1 = (p1 - p1_min) / (p1_max - p1_min)
        p_norm_2 = (p2 - p2_min) / (p2_max - p2_min)
        X = np.concatenate((p_norm_1, p_norm_2))
    else:
        X = np.concatenate((p1, p2))
    W = np.concatenate((w1, w2)).reshape(-1, 1)
    y = np.concatenate((np.zeros(len(p1)), np.ones(len(p2)))).reshape(-1, 1)
    Y = np.append(1 - y, y, axis=1)
    observed = np.dot((W * Y).T, X)

    feature_count = X.sum(axis=0).reshape(1, -1)
    class_prob = (W * Y).mean(axis=0).reshape(1, -1)
    expected = np.dot((class_prob).T, feature_count)

    chi_squared = np.sum((observed - expected) ** 2 / expected, axis=0)
    #chi_squared[np.isnan(chi_squared)] = np.finfo("float64").max

    return chi_squared

"""Simple Method to sample appropriately from weights.

Parameters
----------
weights : array, shape = [n_samples,]
          Weights for the data set.
sample : float, optional.
         1.0 > sample > 0.0: 
             Set is subsampled.
         sample == 1.0:
             Set is maximally sampled without using bootstrapping, meaning the
             distribution suggested by the weights is achieved only by throw-
             ing events away.
         sample > 1.0:
             Set is oversampled using bootstrapping.
             Note: Oversampling will eventually lead to artifacts, so use with
                   caution etc.
Returns
-------
mask : array, shape = [n_samples,]
       Boolean mask that determines whether or not a sample
       has been chosen.
"""
def proper_sampling(weights, sample=1.0):
    probs = sample * weights / np.max(weights)
    rnd = np.random.rand(len(weights))
    idx = np.arange(0, len(weights), dtype=int)
    mask = idx[rnd <= probs]
    probs[probs <= 1.0] = 0.0
    probs[probs > 1.0] -= 1.0
    while(np.sum(probs > 1.0) > 0):
        rnd = np.random.rand(len(weights))
        mask = np.append(mask, idx[rnd <= probs])
        probs[probs <= 1.0] = 0.0
        probs[probs > 1.0] -= 1.0
    return mask

#==============================================================================
# Random Forest Reweighter.
#==============================================================================
class RandomForestReweighter:
    """Random forest that is able to reweight one dataset according to another
    dataset.

    Two datasets, the original and the target ones, are concatenated into one
    dataset and a label for each of the datasets is provided. Then a random
    forest is used to attempt to separate the two populations. The resulting
    scores :math:`s_k` are then used to reweight the original dataset as

    :math:`w' = w \prod_{k=1}^{K} \frac{s_k}{1-s_k}`

    This concludes the first iteration step. After that, an arbitrary number of
    iterations can be performed using the same scheme.

    Parameters
    ----------
    n_estimators : int, optional (default=50)
                   Number of estimators used by the random forest classifier.
                   More than 200 or so estimators will eventually run into
                   numerical problems.

    max_depth : int, optional (default=5)
                Maximum depth of a tree. Using shallow trees usually enhances
                stability and speed. Too shallow trees will achieve nothing at 
                all

    min_samples_leaf : int, optional (default=100)
                       Minimum number of samples in one leaf.

    reg_offset : float, optional (default=1e-3)
                 Offset regularization. Basically account for a minimum weight
                 left in a leaf for both classes. Counteracts overtraining.

    reg_power : float, optional (default=1.0)
                Power regularization. Softens the impact of highly unbalanced
                leaves. Should by in (0.0, 1.0) in order to have any effect.

    n_jobs : int, optional (default=-1)
             Number of jobs for the RandomForestClassifier.

    Attributes
    ----------
    rf : RandomForestClassifier Object
         The Classifier used.

    reg_offset : float
                 Offset regularization

    reg_power : float
                Power regularization

    rw_factors : array, shape=[n_samples_orig,]
                 Reweighting factors
    """
    def __init__(self, 
                 n_estimators=50,
                 max_depth=5,
                 min_samples_leaf=100, 
                 reg_offset=1e-3,
                 reg_power=1.0,
                 n_jobs=-1,
                 crawl_speed=1.0):
        self.rf = RandomForestClassifier(n_estimators=n_estimators,
                                         max_depth=max_depth,
                                         min_samples_leaf=min_samples_leaf,
                                         n_jobs=n_jobs)
        self.reg_offset = reg_offset
        self.reg_power = reg_power
        self.step = 0
        self.crawl_speed = crawl_speed

    def _check_data(self, data):
        if data.__class__.__name__ == "DataFrame":
            return data.as_matrix()
        else:
            return data

    def _predict_crawl(self, X, frac_orig, frac_target):
        concrete_speed = np.mean(self.rw_factors) * self.crawl_speed
        self.rw_factors += (self.rf.predict_proba(X)[:, 1] - 0.5) * concrete_speed

    def _predict_split(self, X, frac_orig, frac_target):
        n = self.rf.n_estimators
        predictions = np.zeros((len(X), n))
        for estimator, i in zip(self.rf.estimators_, range(n)):
            predictions[:, i] = estimator.predict_proba(X)[:, 1]
        fac = (predictions + self.reg_offset) / (1.0 -
                                                 predictions + self.reg_offset)
        bias = float(frac_orig) / float(frac_target)
        self.rw_factors *= np.prod(fac * bias, axis=1) ** (1.0 / float(n))

    def predict_rw_factors(self, data_orig, data_target,
                           sample_orig=1.0,
                           sample_target=1.0,
                           n_iterations=15,
                           weights_orig=None,
                           crawl_mode=False):
        """Fit and predict reweighting factors.
            
            Parameters
            ----------
            data_orig : array, shape = [n_samples_orig,]
                        Original data set to be reweighted.

            data_target : array, shape = [n_samples_target,]
                          Target data set.

            sample_orig : float, optional (default=1.0)
                          Fraction of samples used from the original data set.

            sample_target : float, optional (default=1.0)
                            Fraction of samples used from the target data set.

            n_iterations : int, optional (default=15)
                           Number of iterations.

            weights_orig : None or array, shape = [n_samples_orig,]
                           Weights of the original dataset if exists.

            Returns
            -------
            rw_factors : array, shape = [n_samples_orig,]
                         Reweighting factors.
            """
        dt_o = self._check_data(data_orig)
        dt_t = self._check_data(data_target)
        if self.step == 0:
            if weights_orig is None:
                self.rw_factors = np.ones(len(data_orig))
            else:
                self.rw_factors = np.copy(weights_orig)
        w_mean = np.mean(self.rw_factors)
        orig_sum = np.sum(self.rw_factors)
        N_target = int(len(dt_t) * sample_target)
        N_orig = int(len(dt_o) * sample_orig)
        for i in range(n_iterations):
            orig_picks = np.random.choice(len(dt_o), N_orig, replace=False)
            target_picks = np.random.choice(len(dt_t), N_target, replace=False)
            training_set = np.concatenate((dt_o[orig_picks, :],
                                           dt_t[target_picks, :]))
            training_lab = np.concatenate((np.zeros(len(orig_picks)),
                                           np.ones(len(target_picks))))
            picked_weights = np.concatenate((self.rw_factors[orig_picks],
                                             w_mean * np.ones(len(target_picks)))) 
            self.rf.fit(training_set, training_lab,
                        sample_weight=picked_weights)
            if crawl_mode is False:
                self._predict_split(data_orig, N_orig, N_target)
            else:
                self._predict_crawl(data_orig, N_orig, N_target)
            self.step += 1
            print("Finished iterations %i" % self.step)
        return self.rw_factors
   
    """Set parameters of RandomForestClassifier.

    Parameters
    ----------
    kwargs : dict
             Dictionary of parameters to set.

    Returns
    -------
    self : RandomForestReweighter Object.
           This object.
    """
    def set_params(self, kwargs):
        self.rf.set_params(**kwargs)
        return self


#==============================================================================
# Reweighting Factor Smoothing
#==============================================================================
class RWFactorSmoothing:
    """Smoothes reweighting factors using a running average approach.

    Parameters
    ----------
    k : int, optional (default=10)
        Number of nearest neighbors to average.

    normalize : bool, optional (default=True)
                Whether or not to normalize the input.
    
    Attributes
    ----------
    k : int, optional (default=10)
        Number of nearest neighbors to average.

    normalize : bool, optional (default=True)
                Whether or not to normalize the input.
    """
    def __init__(self, k=10, normalize=True):
        self.k = k
        self.norm = normalize

    def smooth(self, att, rw_weights):
        if self.norm is True:
            mmscaler = MinMaxScaler()
            att_norm = mmscaler.fit_transform(att)
        else:
            att_norm = att
        nearest_neighbors = NearestNeighbors(n_neighbors=self.k)
        nearest_neighbors.fit(att_norm)
        nn_idx = nearest_neighbors.kneighbors(att_norm, n_neighbors=self.k,
                                               return_distance=False)
        rw_weights_new = np.mean(rw_weights[nn_idx], axis=1)
        return rw_weights_new